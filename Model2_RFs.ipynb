{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 - Classification for Pregancy Topics\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2xWRerFdhWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQL7Fh3dBVaL"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import nan\n",
        "df = pd.read_excel(\"Model2.xlsx\")\n",
        "df = df.sample(frac=1, random_state=0)\n",
        "df"
      ],
      "metadata": {
        "id": "F1FQXYteBqIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics of Sentences and Words"
      ],
      "metadata": {
        "id": "qzNYaNGyeHRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "3ZTYNe0md4CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "90iCxLbyedMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "M8Xcv8icehrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(\n",
        "    \"\"\"\n",
        "a about above across after afterwards again against all almost alone along\n",
        "already also although always am among amongst amount an and another any anyhow\n",
        "anyone anything anyway anywhere are around as at\n",
        "back be became because become becomes becoming been before beforehand behind\n",
        "being below beside besides between beyond both bottom but by\n",
        "call can cannot ca could\n",
        "did do does doing done down due during\n",
        "each eight either eleven else elsewhere empty enough even ever every\n",
        "everyone everything everywhere except\n",
        "few fifteen fifty first five for former formerly forty four from front full\n",
        "further\n",
        "get give go\n",
        "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
        "him himself his how however hundred\n",
        "i if in indeed into is it its itself\n",
        "keep\n",
        "last latter latterly least less\n",
        "just\n",
        "made make many may me meanwhile might mine more moreover most mostly move much\n",
        "must my myself\n",
        "name namely neither never nevertheless next nine no nobody none noone nor not\n",
        "nothing now nowhere\n",
        "of off often on once one only onto or other others otherwise our ours ourselves\n",
        "out over own\n",
        "part per perhaps please put\n",
        "quite\n",
        "rather re really regarding\n",
        "same say see seem seemed seeming seems serious several she should show side\n",
        "since six sixty so some somehow someone something sometime sometimes somewhere\n",
        "still such\n",
        "take ten than that the their them themselves then thence there thereafter\n",
        "thereby therefore therein thereupon these they third this those though three\n",
        "through throughout thru thus to together too top toward towards twelve twenty\n",
        "two\n",
        "under until up unless upon us used using\n",
        "various very very via was we well were what whatever when whence whenever where\n",
        "whereafter whereas whereby wherein whereupon wherever whether which while\n",
        "whither who whoever whole whom whose why will with within without would\n",
        "yet you your yours yourself yourselves\n",
        "\"\"\".split()\n",
        ")\n",
        "\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "id": "7JiHebIpi-WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Callable that can be passed to the Vectorizer \n",
        "class LemmaTokenizer:\n",
        "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`','--']\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]"
      ],
      "metadata": {
        "id": "jw_blFb6jDxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the stop words and store them in a list so that list can be passed to Vectorizer\n",
        "tokenizer = LemmaTokenizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# Storing all stop words in a list \n",
        "token_stop = tokenizer(' '.join(stop_words))"
      ],
      "metadata": {
        "id": "zgExAjRtjHVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution by fold and tag\n",
        "fold_crosstab = pd.crosstab(index=df[\"Tag\"],columns=df[\"fold\"],margins=True)\n",
        "fold_crosstab"
      ],
      "metadata": {
        "id": "nPCBA6f_kOla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = df[(df['fold'] != 1)].title.tolist()\n",
        "x_test = df[df['fold'] == 1].title.tolist()\n",
        "y_train = df[(df['fold'] != 1)].Tag.tolist()\n",
        "y_test = df[df['fold'] == 1].Tag.tolist()\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(x_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "metadata": {
        "id": "o5XMuWpokgWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "qdZy-nNRkyuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# We are excluding words which have very minimal frequency (< 4)\n",
        "# Tune as needed after receiving feedback\n",
        "# Computing unigrams and bigrams\n",
        "# Capping Dimensionality at 1500 features\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=token_stop, \n",
        "                                   tokenizer = tokenizer, \n",
        "                                   ngram_range = (1,2), \n",
        "                                   min_df = 1,\n",
        "                                   max_features = 1500)\n",
        "\n",
        "X = tfidf_vectorizer.fit_transform(x_train)\n",
        "print('Training documents have', len(tfidf_vectorizer.vocabulary_), 'words')\n",
        "print('X:', X.shape)"
      ],
      "metadata": {
        "id": "Hxmj189Mkxnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "WPzeoP2zlACG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "RFC = RandomForestClassifier(n_estimators = 200, \n",
        "                             criterion = 'gini', \n",
        "                             random_state = 0,\n",
        "                             n_jobs = - 1)\n",
        "RFC.fit(X,y_train)"
      ],
      "metadata": {
        "id": "6OIZK7dkk9dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tfidf_vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "MR7nm6XflHD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = RFC.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "pre, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
        "print('Precision: ' , pre)\n",
        "print('Recall:    ' , recall)\n",
        "print('F1:        ' , f1)"
      ],
      "metadata": {
        "id": "PDaHkOXjlM1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cf_matrix =  confusion_matrix(y_test, y_pred, labels=[1,0,3,4])\n",
        "\n",
        "# Presenting the Confusion Matrix as a Dataframe\n",
        "# Horizontal Labels = Predicted Labels\n",
        "# Vertical Labels   = Ground Truth Labels\n",
        "pd.DataFrame(cf_matrix, index=[\"Diet\", \"Exercise\", \"Mental Health\", \"Other\"], \n",
        "             columns=[\"Diet\", \"Exercise\", \"Mental Health\", \"Other\"])"
      ],
      "metadata": {
        "id": "L7--cdJPla4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, labels=[1,0,3,4], \n",
        "                            digits = 4,\n",
        "                            target_names=[\"Diet\", \"Exercise\", \"Mental Health\", \"Other\"]))"
      ],
      "metadata": {
        "id": "PckVvL0E6SAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}