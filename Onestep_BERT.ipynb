{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5beb1487-ec14-45a0-bd83-3eaf64c43cb2",
      "metadata": {
        "id": "5beb1487-ec14-45a0-bd83-3eaf64c43cb2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, bert, num_class=5):\n",
        "        super(BERT, self).__init__()\n",
        "        self.bert = bert \n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512, num_class)\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, input_ids, masks):\n",
        "        #pass the inputs to the model  \n",
        "        input_ids = input_ids.squeeze(1)\n",
        "        masks = masks.squeeze(1)\n",
        "        \n",
        "        cls_hs = self.bert(input_ids, masks).pooler_output\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04d689d-b514-4974-9285-b993875d1a1a",
      "metadata": {
        "id": "f04d689d-b514-4974-9285-b993875d1a1a"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, df):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts, self.labels = [], []\n",
        "        \n",
        "        for idx, row in df.iterrows():\n",
        "            label = row['label']\n",
        "            text = str(row['text'])\n",
        "\n",
        "            self.labels.append(label)\n",
        "            self.texts.append(self.tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43681cd1-19cc-44ed-b7d2-fac70c3742ad",
      "metadata": {
        "id": "43681cd1-19cc-44ed-b7d2-fac70c3742ad"
      },
      "outputs": [],
      "source": [
        "def evaluate(test_dataloader, model, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    true_list, pred_list = [], []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        input_ids, type_ids, masks, labels = batch[0]['input_ids'].to(device), batch[0]['token_type_ids'].to(device), batch[0]['attention_mask'].to(device), batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad(): \n",
        "            # get model predictions for the current batch\n",
        "            preds = model(input_ids, masks)\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().tolist()\n",
        "            \n",
        "            for idx in range(len(labels)):\n",
        "                label = labels[idx]\n",
        "                true_list.append(label)\n",
        "                pred_list.append(np.argmax(preds[idx]))\n",
        "\n",
        "    print(pred_list)\n",
        "    print('Epoch {}, Precision: {}, Recall: {}, F-1: {}\\n'.format(epoch, precision, recall, f1))\n",
        "    print(confusion_matrix(true_list, pred_list, labels=[1,0,3,2,4]))\n",
        "    print(classification_report(true_list, pred_list,labels=[1,0,3,2,4],\n",
        "                                digits = 4,\n",
        "                                target_names=[\"Diet\", \"Exercise\", \"Mental Health\", \"Other\",\"Unrelated\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d598dad-cb16-45b7-b4d3-3ca418244aa1",
      "metadata": {
        "id": "3d598dad-cb16-45b7-b4d3-3ca418244aa1"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, test_dataloader, model, tokenizer, criterion, device):\n",
        "    \n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    # empty list to save model predictions\n",
        "    pred_labels, true_labels = [], []\n",
        "    best_epoch, best_f1 = 0, -100\n",
        "    \n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        print('Training, Epoch: {}'.format(epoch))\n",
        "        total_loss = 0\n",
        "        # iterate over batches\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            input_ids, type_ids, masks, labels = batch[0]['input_ids'].to(device), batch[0]['token_type_ids'].to(device), batch[0]['attention_mask'].to(device), batch[1].to(device)\n",
        "\n",
        "            # clear previously calculated gradients \n",
        "            model.zero_grad()        \n",
        "            preds = model(input_ids, masks)\n",
        "            loss = criterion(preds, labels)\n",
        "            total_loss = total_loss + loss.item()\n",
        "            \n",
        "            # update parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(total_loss/len(train_dataloader))\n",
        "        evaluate(test_dataloader, model, criterion, device, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a1aa51-e6c6-4379-8d45-7890eb6c5edd",
      "metadata": {
        "id": "e2a1aa51-e6c6-4379-8d45-7890eb6c5edd"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = 'onestep.csv'# path to your data .csv file\n",
        "\n",
        "\n",
        "MODELS = {'PubMedBERT': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
        "          'BioBERT': 'dmis-lab/biobert-base-cased-v1.1',\n",
        "          'ClinicalBERT': 'emilyalsentzer/Bio_ClinicalBERT'}\n",
        "\n",
        "MODEL = 'BioBERT' # Specify the pre-trained BERT model to use\n",
        "LEARNING_RATE = 2e-5\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00391bf-65d9-44d4-a8eb-95f955a4825f",
      "metadata": {
        "id": "d00391bf-65d9-44d4-a8eb-95f955a4825f"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Device:', device)\n",
        "\n",
        "# initialize BERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODELS[MODEL])\n",
        "bert = AutoModel.from_pretrained(MODELS[MODEL])\n",
        "model = BERT(bert, num_class=5) # specify how many classes to classify, here for example we want to classify 5 classes\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3ea3cf-c58e-4a4f-a809-9b8bac6dead3",
      "metadata": {
        "id": "2b3ea3cf-c58e-4a4f-a809-9b8bac6dead3"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "df = pd.read_csv(DATA_DIR)\n",
        "\n",
        "train_df = df[df['fold'] != 1]\n",
        "test_df = df[df['fold'] == 1]\n",
        "\n",
        "\n",
        "train_df = train_df.sample(frac=1, random_state=0)\n",
        "#sample train \n",
        "    \n",
        "train_dataset = Dataset(tokenizer, train_df)\n",
        "test_dataset = Dataset(tokenizer, test_df)\n",
        "    \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
        "print('*'*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c60f4bf-e23e-42be-b6ef-60a39f825710",
      "metadata": {
        "id": "9c60f4bf-e23e-42be-b6ef-60a39f825710"
      },
      "outputs": [],
      "source": [
        "train(train_dataloader, test_dataloader, model, tokenizer, criterion, device)"
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.g4dn.8xlarge",
    "kernelspec": {
      "display_name": "Python 3 (Data Science)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}